{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpiRRD1IrqOZuD3zVuolI8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/strateg17/fake-news/blob/dev/Balaka_model_selection_Optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nZz5Q8Ijeqo"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from typing import Tuple, Dict, Callable, Any\n",
        "\n",
        "def load_and_prepare_data() -> Tuple[pd.Series, pd.Series, pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Load and prepare data for training and testing.\n",
        "\n",
        "    Returns:\n",
        "        X_train, X_test, y_train, y_test: Training and testing features and labels.\n",
        "    \"\"\"\n",
        "    # Load the data\n",
        "    df_test = pd.read_parquet('/content/english_fact_test.parquet', columns=['claim', 'label'])\n",
        "    df_train = pd.read_parquet('/content/english_fact_train.parquet', columns=['claim', 'label'])\n",
        "\n",
        "    # Combine datasets\n",
        "    df_combined = pd.concat([df_test, df_train], ignore_index=True)\n",
        "\n",
        "    # Map labels to numeric values\n",
        "    label_mapping = {'Supported': 0, 'Refuted': 1}\n",
        "    df_combined['label'] = df_combined['label'].map(label_mapping)\n",
        "\n",
        "    # Remove rows with NaN labels\n",
        "    df_combined = df_combined.dropna(subset=['label'])\n",
        "\n",
        "    # Convert labels to integer type\n",
        "    df_combined['label'] = df_combined['label'].astype(int)\n",
        "\n",
        "    # Split features and labels\n",
        "    X = df_combined.claim\n",
        "    y = df_combined.label\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def tfidf_transform(X_train: list[str], X_test: list[str]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Transform text data into TF-IDF vectors.\n",
        "\n",
        "    Args:\n",
        "        X_train (list[str]): Training texts.\n",
        "        X_test (list[str]): Testing texts.\n",
        "\n",
        "    Returns:\n",
        "        X_train_tfidf, X_test_tfidf: Transformed training and testing data as TF-IDF vectors.\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "    return X_train_tfidf, X_test_tfidf\n",
        "\n",
        "def objective(trial) -> float:\n",
        "    \"\"\"\n",
        "    Objective function for Optuna to optimize LightGBM hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        trial: An Optuna trial object.\n",
        "\n",
        "    Returns:\n",
        "        Accuracy score of the model with the trial's hyperparameters.\n",
        "    \"\"\"\n",
        "    # Define the hyperparameters to tune\n",
        "    param = {\n",
        "        'objective': 'binary',\n",
        "        'metric': 'accuracy',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
        "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 100),\n",
        "        'num_boost_round': trial.suggest_int('num_boost_round', 50, 1000),\n",
        "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
        "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
        "        'min_gain_to_split': trial.suggest_loguniform('min_gain_to_split', 0.1, 10.0),\n",
        "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
        "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0)\n",
        "    }\n",
        "\n",
        "    # Transform the text data using TF-IDF\n",
        "    X_train_tfidf, X_test_tfidf = tfidf_transform(X_train, X_test)\n",
        "\n",
        "    # Train and evaluate the LightGBM model\n",
        "    model = LGBMClassifier(**param)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    return accuracy_score(y_test, y_pred)\n",
        "\n",
        "def optimize_hyperparameters() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Optimize LightGBM hyperparameters using Optuna and return the best parameters.\n",
        "\n",
        "    Returns:\n",
        "        best_params: Best hyperparameters found by Optuna.\n",
        "    \"\"\"\n",
        "    # Create an Optuna study\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=100)\n",
        "\n",
        "    # Print the best trial results\n",
        "    print('Best trial:')\n",
        "    trial = study.best_trial\n",
        "    print(f'  Value: {trial.value}')\n",
        "    print('  Params: ')\n",
        "    for key, value in trial.params.items():\n",
        "        print(f'    {key}: {value}')\n",
        "\n",
        "    return study.best_params\n",
        "\n",
        "def train_and_evaluate_model(best_params: Dict[str, Any], X_train: list[str], X_test: list[str], y_train: pd.Series, y_test: pd.Series) -> None:\n",
        "    \"\"\"\n",
        "    Train and evaluate the LightGBM model with the best hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        best_params (Dict[str, Any]): Best hyperparameters found by Optuna.\n",
        "        X_train (list[str]): Training texts.\n",
        "        X_test (list[str]): Testing texts.\n",
        "        y_train (pd.Series): Training labels.\n",
        "        y_test (pd.Series): Testing labels.\n",
        "    \"\"\"\n",
        "    # Transform the text data using TF-IDF\n",
        "    X_train_tfidf, X_test_tfidf = tfidf_transform(X_train, X_test)\n",
        "\n",
        "    # Train the model with the best parameters\n",
        "    best_model = LGBMClassifier(**best_params)\n",
        "    best_model.fit(X_train_tfidf, y_train)\n",
        "    y_pred_train = best_model.predict(X_train_tfidf)\n",
        "    y_pred_test = best_model.predict(X_test_tfidf)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    # Print accuracy results\n",
        "    print(f\"Точність на навчальній вибірці: {train_accuracy:.4f}\")\n",
        "    print(f\"Точність на тестовій вибірці: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Plot training and testing errors\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(['Train Error', 'Test Error'], [1 - train_accuracy, 1 - test_accuracy], color=['blue', 'orange'])\n",
        "    plt.title('Навчальні і тестові помилки для LightGBM з використанням TF-IDF')\n",
        "    plt.ylabel('Помилка')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the workflow: data loading, hyperparameter optimization,\n",
        "    and model training and evaluation.\n",
        "    \"\"\"\n",
        "    # Load and prepare data\n",
        "    X_train, X_test, y_train, y_test = load_and_prepare_data()\n",
        "\n",
        "    # Optimize hyperparameters\n",
        "    best_params = optimize_hyperparameters()\n",
        "\n",
        "    # Train and evaluate the model with the best hyperparameters\n",
        "    train_and_evaluate_model(best_params, X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}